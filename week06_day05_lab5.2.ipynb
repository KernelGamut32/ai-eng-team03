{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ee1540",
   "metadata": {},
   "source": [
    "# Day 5 Mini-Capstone Lab  \n",
    "## End-to-End Evaluation & Tracking for a HomePro LLM App (TruLens + MLflow)\n",
    "\n",
    "**Estimated duration:** ~240 minutes (4 hours)  \n",
    "**Theme:** Design and implement an *end-to-end* evaluation strategy for a small LLM-backed application in a **retail home improvement** domain using:\n",
    "\n",
    "- Traditional metrics: **accuracy, F1, BLEU, ROUGE**\n",
    "- An **evaluation harness** that runs your model over a labeled test set\n",
    "- **MLflow** for experiment tracking (metrics, params, artifacts)\n",
    "- **TruLens** for structured LLM evaluation (groundedness, answer relevance, context relevance)\n",
    "- A local LLM running in **LM Studio** (OpenAI-compatible API)\n",
    "\n",
    "By the end of this lab you should be able to:\n",
    "\n",
    "1. Build a synthetic **HomePro**-style corpus (~500 docs) with realistic variation.  \n",
    "2. Define an **evaluation dataset** for at least one of:\n",
    "   - FAQ answerer (RAG-style Q&A)  \n",
    "   - Summarizer (summarize product/policy docs)  \n",
    "   - Sentiment classifier (label customer reviews)  \n",
    "3. Implement a simple **RAG-style LLM app** over that corpus using LM Studio.  \n",
    "4. Build a **metric harness** to compute accuracy/F1 (for classification), BLEU, and ROUGE.  \n",
    "5. Track **two variants** of your app with MLflow and compare them.  \n",
    "6. Instrument your app with **TruLens** and compute structured LLM metrics:\n",
    "   - Groundedness (faithfulness)\n",
    "   - Answer relevance\n",
    "   - Context relevance  \n",
    "7. Combine **MLflow** + **TruLens** outputs to argue which variant is “better” and why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4c590",
   "metadata": {},
   "source": [
    "## 0. Pre-requisites & Environment Setup\n",
    "\n",
    "### 0.1. LM Studio (local LLM)\n",
    "\n",
    "You will use **LM Studio** as an on-box LLM, exposed via an **OpenAI-compatible HTTP API**.\n",
    "\n",
    "1. Open LM Studio and download a reasonably capable chat model (e.g., a 7B–8B instruction-tuned model).  \n",
    "2. Start the **local server** in LM Studio (UI: *\"Local Server\"* → *\"Start Server\"*).  \n",
    "   - Default base URL (check LM Studio docs / status panel):\n",
    "     - Typically something like: `http://localhost:1234/v1`\n",
    "3. Note the **model ID** you want to use (e.g., `gpt-4o-mini`, `openai/gpt-4o-mini`, or the specific LM Studio model name).\n",
    "\n",
    "We will configure the Python `openai` client (used by TruLens) to talk to this local server via environment variables.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.2. MLflow via Docker (local tracking server)\n",
    "\n",
    "We’ll run an **MLflow tracking server** locally in Docker and log experiments to it.\n",
    "\n",
    "**From a terminal in your lab project directory:**\n",
    "\n",
    "```bash\n",
    "# If not already existing\n",
    "mkdir -p $PWD/mlflow_data/mlruns\n",
    "mkdir -p $PWD/mlflow_data/mlartifacts\n",
    "\n",
    "docker run --rm -it -d -p 5000:5000 -v \"$(pwd)/mlflow_data/mlruns:/mlflow/mlruns\" -v \"$(pwd)/mlflow_data/mlartifacts:/mlflow/artifacts\" ghcr.io/mlflow/mlflow:latest mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri \"sqlite:///mlflow.db\" --default-artifact-root \"/mlflow/artifacts\"\n",
    "```\n",
    "\n",
    "- In the browser, open: <http://localhost:5000> to see the MLflow UI.\n",
    "- You will log runs to this server from the notebook using `MLFLOW_TRACKING_URI=http://localhost:5000`.\n",
    "\n",
    "---\n",
    "\n",
    "### 0.3. Python environment & dependencies\n",
    "\n",
    "This notebook assumes a Python 3.13+ virtual environment with `pip` available.\n",
    "\n",
    "In the kernel being used for the notebook, install the core dependencies (if not already installed):\n",
    "\n",
    "```bash\n",
    "pip install mlflow scikit-learn sacrebleu rouge-score pandas numpy matplotlib openai trulens trulens-providers-openai\n",
    "```\n",
    "\n",
    "> **Note on CUDA vs Apple Silicon (MPS)**  \n",
    "> - LM Studio handles GPU selection for you.  \n",
    "> - On **NVIDIA/CUDA** machines, choose a CUDA-capable model in LM Studio.  \n",
    "> - On **Apple Silicon**, choose an MPS-capable model in LM Studio.  \n",
    "> - This notebook itself does not require GPU-specific code; any heavy lifting is done by LM Studio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243616fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow scikit-learn sacrebleu rouge-score pandas numpy matplotlib openai trulens trulens-providers-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# === LM Studio / OpenAI-compatible configuration ===\n",
    "#\n",
    "# Adjust these if your LM Studio server uses a different base URL or port.\n",
    "# Common default from LM Studio docs is http://localhost:1234/v1\n",
    "\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"lm-studio\")  # Dummy key; LM Studio ignores it.\n",
    "os.environ.setdefault(\"OPENAI_BASE_URL\", \"http://localhost:1234/v1\")\n",
    "# Some libraries still look for OPENAI_API_BASE; mirror the value for robustness.\n",
    "os.environ.setdefault(\"OPENAI_API_BASE\", os.environ[\"OPENAI_BASE_URL\"])\n",
    "\n",
    "# Choose a default model ID; you can override this later in the lab.\n",
    "DEFAULT_LM_STUDIO_MODEL = os.getenv(\"LM_STUDIO_MODEL\", \"openai/gpt-oss-20b\")\n",
    "\n",
    "print(\"Configured LM Studio-compatible OpenAI client:\")\n",
    "print(\"  OPENAI_BASE_URL =\", os.environ[\"OPENAI_BASE_URL\"])\n",
    "print(\"  OPENAI_API_KEY  =\", os.environ[\"OPENAI_API_KEY\"])\n",
    "print(\"  Default model    =\", DEFAULT_LM_STUDIO_MODEL)\n",
    "\n",
    "# === MLflow tracking server configuration ===\n",
    "#\n",
    "# Make sure the Docker-based MLflow server is running on localhost:5000 before continuing.\n",
    "\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://localhost:5000\")\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "print(\"Configured MLflow tracking URI:\", mlflow.get_tracking_uri())\n",
    "\n",
    "# Set up local artifact storage path\n",
    "REPO_ROOT = Path.cwd()\n",
    "LOCAL_ARTIFACT_ROOT = REPO_ROOT / \"mlflow_data\" / \"mlartifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69577887",
   "metadata": {},
   "source": [
    "## 1. Build a Synthetic HomePro Corpus (~500 docs)\n",
    "\n",
    "In this section you will:\n",
    "\n",
    "- Generate a synthetic **HomePro**-style corpus of ~500 short documents.  \n",
    "- Cover multiple categories (flooring, paint, lighting, cabinets, appliances, garden, policies, services).  \n",
    "- Include **variation** so evaluation metrics won’t be “too perfect”: overlapping info, messy reviews, promotions, policies, etc.\n",
    "\n",
    "We’ll represent the corpus as a `pandas.DataFrame` with columns like:\n",
    "\n",
    "- `doc_id`: integer ID  \n",
    "- `category`: product or topic category  \n",
    "- `doc_type`: type of content (product_guide, policy, installation, troubleshooting, promotion, review)  \n",
    "- `text`: the actual text chunk used for retrieval / summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "categories = [\n",
    "    \"laminate flooring\",\n",
    "    \"vinyl plank flooring\",\n",
    "    \"ceramic tile\",\n",
    "    \"interior paint\",\n",
    "    \"exterior paint\",\n",
    "    \"LED lighting\",\n",
    "    \"kitchen cabinets\",\n",
    "    \"bathroom vanities\",\n",
    "    \"power tools\",\n",
    "    \"garden & outdoor\",\n",
    "]\n",
    "\n",
    "doc_types = [\n",
    "    \"product_guide\",\n",
    "    \"policy_faq\",\n",
    "    \"installation_guide\",\n",
    "    \"troubleshooting\",\n",
    "    \"promotion\",\n",
    "    \"customer_review\",\n",
    "]\n",
    "\n",
    "warranty_terms = [\"1-year\", \"2-year\", \"3-year\", \"5-year\", \"10-year\"]\n",
    "return_windows = [\"30 days\", \"60 days\", \"90 days\"]\n",
    "finish_levels = [\"matte\", \"eggshell\", \"satin\", \"semi-gloss\"]\n",
    "traffic_levels = [\"light\", \"medium\", \"heavy\"]\n",
    "\n",
    "review_sentiments = [\"positive\", \"negative\", \"neutral\"]\n",
    "review_phrases_positive = [\n",
    "    \"loved how easy the install was\",\n",
    "    \"customer service was fantastic\",\n",
    "    \"quality felt better than expected\",\n",
    "    \"installer arrived on time and cleaned up thoroughly\",\n",
    "    \"the color matched the sample perfectly\",\n",
    "]\n",
    "review_phrases_negative = [\n",
    "    \"installation was delayed and communication was poor\",\n",
    "    \"finish scratched more easily than expected\",\n",
    "    \"return process felt confusing and slow\",\n",
    "    \"instructions were unclear and missing steps\",\n",
    "    \"delivery arrived late and boxes were damaged\",\n",
    "]\n",
    "review_phrases_neutral = [\n",
    "    \"product quality was fine, nothing special\",\n",
    "    \"store was crowded but staff eventually helped\",\n",
    "    \"color was close enough to what we expected\",\n",
    "    \"installer did the job, but scheduling took a few calls\",\n",
    "    \"overall experience was acceptable but not memorable\",\n",
    "]\n",
    "\n",
    "docs = []\n",
    "doc_id = 0\n",
    "\n",
    "# Target ~500 docs: 10 categories * ~50 docs each = 500\n",
    "docs_per_category = 50\n",
    "\n",
    "for category in categories:\n",
    "    for i in range(docs_per_category):\n",
    "        d_type = random.choice(doc_types)\n",
    "\n",
    "        # Random knobs to create variation\n",
    "        warranty = random.choice(warranty_terms)\n",
    "        return_window = random.choice(return_windows)\n",
    "        finish = random.choice(finish_levels)\n",
    "        traffic = random.choice(traffic_levels)\n",
    "\n",
    "        if d_type == \"product_guide\":\n",
    "            text = (\n",
    "                f\"HomePro offers several {category} options designed for {traffic} traffic areas. \"\n",
    "                f\"Most {category} products come with a {warranty} limited residential warranty when \"\n",
    "                f\"installed according to the manufacturer guidelines. Customers should check the \"\n",
    "                f\"subfloor, moisture levels, and acclimation requirements before starting. For finishes, \"\n",
    "                f\"many customers prefer a {finish} finish to balance durability and appearance. \"\n",
    "                f\"HomePro stores typically keep popular colors and textures in stock but special orders \"\n",
    "                f\"may take 7–14 days depending on the vendor.\"\n",
    "            )\n",
    "        elif d_type == \"policy_faq\":\n",
    "            text = (\n",
    "                f\"HomePro's return policy for {category} is designed to be flexible while protecting product quality. \"\n",
    "                f\"Most unopened {category} can be returned within {return_window} with proof of purchase. \"\n",
    "                f\"Cut-to-length or custom-tinted products may not be returnable. For installed {category}, \"\n",
    "                f\"HomePro recommends contacting the installation support team before removal to avoid damage and safety issues. \"\n",
    "                f\"Refunds are generally issued to the original payment method; store credit is available for certain promotions.\"\n",
    "            )\n",
    "        elif d_type == \"installation_guide\":\n",
    "            text = (\n",
    "                f\"When installing {category}, HomePro recommends dry-fitting a small area before committing to full adhesive coverage. \"\n",
    "                f\"For high-{traffic} areas, customers should use premium underlayment and follow the expansion-gap guidelines \"\n",
    "                f\"listed on the packaging. Surfaces must be clean, flat, and structurally sound. \"\n",
    "                f\"For {finish} finishes, customers should avoid harsh cleaners for the first 7 days after installation. \"\n",
    "                f\"HomePro provides printed installation guides in-store and video tutorials in the online learning center.\"\n",
    "            )\n",
    "        elif d_type == \"troubleshooting\":\n",
    "            text = (\n",
    "                f\"Common issues with {category} purchased at HomePro include minor color variation between batches and \"\n",
    "                f\"surface noise in high-{traffic} hallways. Color variation can often be minimized by mixing planks or tiles \"\n",
    "                f\"from multiple boxes during installation. For squeaks or hollow sounds, customers should verify subfloor prep \"\n",
    "                f\"and ensure that recommended underlayment was used. If problems persist, HomePro's installation support line \"\n",
    "                f\"can review photos and suggest remedies or warranty options.\"\n",
    "            )\n",
    "        elif d_type == \"promotion\":\n",
    "            text = (\n",
    "                f\"During the seasonal HomePro Savings Event, select {category} products may qualify for bundled discounts. \"\n",
    "                f\"Typical offers include percentage discounts when purchasing a minimum square footage, or free delivery \"\n",
    "                f\"on qualifying orders. Promotions on {category} sometimes combine with installation offers, such as \"\n",
    "                f\"discounted labor on weekday installs. Customers should review fine print, as clearance items and certain \"\n",
    "                f\"premium collections might be excluded.\"\n",
    "            )\n",
    "        else:  # customer_review\n",
    "            sentiment = random.choice(review_sentiments)\n",
    "            if sentiment == \"positive\":\n",
    "                phrase = random.choice(review_phrases_positive)\n",
    "            elif sentiment == \"negative\":\n",
    "                phrase = random.choice(review_phrases_negative)\n",
    "            else:\n",
    "                phrase = random.choice(review_phrases_neutral)\n",
    "\n",
    "            text = (\n",
    "                f\"I recently purchased {category} from HomePro. Overall, I would describe my experience as {sentiment}. \"\n",
    "                f\"I {phrase}. The pricing felt {'fair' if sentiment != 'negative' else 'higher than I expected'}, \"\n",
    "                f\"and I {'would' if sentiment == 'positive' else 'might'} shop here again for future projects. \"\n",
    "                f\"The store team {'answered most of my questions' if sentiment != 'negative' else 'seemed rushed and hard to flag down'}. \"\n",
    "                f\"Installation scheduling was {'smooth' if sentiment == 'positive' else 'a little bumpy but acceptable'}.\"\n",
    "            )\n",
    "\n",
    "        docs.append(\n",
    "            {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"category\": category,\n",
    "                \"doc_type\": d_type,\n",
    "                \"text\": text,\n",
    "            }\n",
    "        )\n",
    "        doc_id += 1\n",
    "\n",
    "corpus_df = pd.DataFrame(docs)\n",
    "len_corpus = len(corpus_df)\n",
    "print(f\"Corpus size: {len_corpus} documents\")\n",
    "display(corpus_df.head())\n",
    "display(corpus_df[\"doc_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd6fe8",
   "metadata": {},
   "source": [
    "## 2. Build an Evaluation Dataset\n",
    "\n",
    "We’ll construct a labeled evaluation set:\n",
    "\n",
    "1. **FAQ Answerer (RAG)**  \n",
    "   - Input: customer question (e.g., *“What is the return policy for laminate flooring?”*).  \n",
    "   - Reference: short “gold” answer derived from the corpus.  \n",
    "   - Metrics: BLEU, ROUGE, token-level F1, plus TruLens groundedness/relevance.\n",
    "\n",
    "To keep the lab focused on evaluation (not data engineering), we’ll generate the evaluation rows **programmatically** from the corpus templates. You can extend or edit this to better match your goals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eval_rows = []\n",
    "\n",
    "# --- FAQ-style Q&A examples ---\n",
    "faq_count = 80\n",
    "faq_candidates = corpus_df[corpus_df[\"doc_type\"].isin([\"policy_faq\", \"product_guide\"])].sample(\n",
    "    n=min(faq_count, len(corpus_df)), random_state=123\n",
    ")\n",
    "\n",
    "for idx, row in faq_candidates.iterrows():\n",
    "    category = row[\"category\"]\n",
    "    base_text = row[\"text\"]\n",
    "\n",
    "    # Simple templated questions derived from category\n",
    "    question_templates = [\n",
    "        f\"What is the return policy for {category} at HomePro?\",\n",
    "        f\"How long is the typical warranty on {category} from HomePro?\",\n",
    "        f\"What should I know before installing {category}?\",\n",
    "    ]\n",
    "    question = random.choice(question_templates)\n",
    "\n",
    "    # Reference answers: short, focused, and derived from the text template semantics\n",
    "    ref_answer = (\n",
    "        f\"For {category}, HomePro usually offers a limited warranty and a time-bound return window. \"\n",
    "        f\"Unopened items can often be returned within a specific number of days with a receipt, while \"\n",
    "        f\"custom or cut-to-length products may not be returnable. Installation should follow the \"\n",
    "        f\"product guidelines around subfloor prep, moisture checks, and expansion gaps.\"\n",
    "    )\n",
    "\n",
    "    eval_rows.append(\n",
    "        {\n",
    "            \"task_type\": \"faq\",\n",
    "            \"category\": category,\n",
    "            \"source_doc_id\": row[\"doc_id\"],\n",
    "            \"input_text\": question,\n",
    "            \"reference_answer\": ref_answer,\n",
    "            \"sentiment_label\": None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "eval_df = pd.DataFrame(eval_rows)\n",
    "print(f\"Evaluation set size: {len(eval_df)} examples\")\n",
    "display(eval_df.head())\n",
    "display(eval_df[\"task_type\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
